{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Pjmz-RORV8E"
   },
   "source": [
    "# Extract text from documents\n",
    "\n",
    "Up to this point, all the examples have been working with sections of text, which have already been split through some other means. What happens if we're working with documents? First we need to get the text out of these documents, then figure out how to index to best support vector search.\n",
    "\n",
    "This notebook shows how documents can have text extracted and split to support vector search and retrieval augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk31rbYjSTYm"
   },
   "source": [
    "# Install dependencies\n",
    "\n",
    "Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XMQuuun2R06J"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtar -xvzf tests.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Install NLTK\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      9\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt_tab\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: git+https://github.com/neuml/txtai#egg=txtai[pipeline] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neuml/txtai 'C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\txtai_11fd328dc41b46e58efe86f892910239'\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for fasttext (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [105 lines of output]\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\setuptools\\dist.py:488: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2025-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self.warn_dash_deprecation(opt, section)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\fasttext\n",
      "      copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-cpython-311\\fasttext\n",
      "      copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\n",
      "      creating build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "      copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "      copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "      creating build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "      running build_ext\n",
      "      building 'fasttext_pybind' extension\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\\python\\fasttext_module\\fasttext\\pybind\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\\src\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include -IC:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include -Isrc -IC:\\Users\\Harrison\\anaconda3\\envs\\brandon\\include -IC:\\Users\\Harrison\\anaconda3\\envs\\brandon\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\" /EHsc /Tppython/fasttext_module/fasttext/pybind/fasttext_pybind.cc /Fobuild\\temp.win-amd64-cpython-311\\Release\\python/fasttext_module/fasttext/pybind/fasttext_pybind.obj /EHsc /DVERSION_INFO=\\\\\\\"0.9.3\\\\\\\"\n",
      "      fasttext_pybind.cc\n",
      "      The contents of <string_view> are available only with C++17 or later.\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(40): error C2039: 'string_view': is not a member of 'std'\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\unordered_map(24): note: see declaration of 'std'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(40): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(41): error C2039: 'string_view': is not a member of 'std'\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\unordered_map(24): note: see declaration of 'std'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(41): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(46): error C2039: 'string_view': is not a member of 'std'\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\unordered_map(24): note: see declaration of 'std'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(46): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(75): error C2039: 'string_view': is not a member of 'std'\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\unordered_map(24): note: see declaration of 'std'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(75): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(76): error C2039: 'string_view': is not a member of 'std'\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\unordered_map(24): note: see declaration of 'std'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(76): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(78): error C2039: 'string_view': is not a member of 'std'\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\unordered_map(24): note: see declaration of 'std'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(78): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(78): error C2535: 'fasttext::entry_type fasttext::Dictionary::getType(int32_t) const': member function already defined or declared\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(77): note: see declaration of 'fasttext::Dictionary::getType'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): error C2039: 'string_view': is not a member of 'std'\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\unordered_map(24): note: see declaration of 'std'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): error C2146: syntax error: missing ')' before identifier 'str'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): error C3646: 'str': unknown override specifier\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): error C2059: syntax error: ')'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): error C2143: syntax error: missing ';' before 'const'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): error C2208: 'const int': no members defined using this type\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): warning C4091: ' ': ignored on left of 'const int' when no variable is declared\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(103): error C2039: 'string_view': is not a member of 'std'\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\unordered_map(24): note: see declaration of 'std'\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(103): error C2061: syntax error: identifier 'string_view'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(71): error C2662: 'uint32_t fasttext::Dictionary::hash(const int)': cannot convert 'this' pointer from '_Ty2' to 'fasttext::Dictionary &'\n",
      "              with\n",
      "              [\n",
      "                  _Ty2=const fasttext::Dictionary\n",
      "              ]\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(71): note: Conversion loses qualifiers\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(91): note: see declaration of 'fasttext::Dictionary::hash'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(72): error C2664: 'int32_t fasttext::Dictionary::getId(const int,uint32_t) const': cannot convert argument 1 from 'std::string' to 'const int'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(72): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be called\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(76): note: see declaration of 'fasttext::Dictionary::getId'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(73): error C2664: 'fasttext::entry_type fasttext::Dictionary::getType(int32_t) const': cannot convert argument 1 from 'std::string' to 'int32_t'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(73): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be called\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-install-z2af2c_a\\fasttext_0af22130599a43e58c4ecf4848025c40\\src\\dictionary.h(77): note: see declaration of 'fasttext::Dictionary::getType'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(172): error C2065: 'ssize_t': undeclared identifier\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(172): error C2672: 'pybind11::init': no matching overloaded function found\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): error C2974: 'pybind11::init': invalid template argument for 'CFunc', type expected\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include\\pybind11\\pybind11.h(1978): note: see declaration of 'pybind11::init'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): error C2974: 'pybind11::init': invalid template argument for 'Func', type expected\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include\\pybind11\\pybind11.h(1970): note: see declaration of 'pybind11::init'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): error C2974: 'pybind11::init': invalid template argument for 'Args', type expected\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include\\pybind11\\pybind11.h(1958): note: see declaration of 'pybind11::init'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(172): error C2672: 'pybind11::class_<fasttext::Vector>::def': no matching overloaded function found\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): error C2780: 'pybind11::class_<fasttext::Vector> &pybind11::class_<fasttext::Vector>::def(const char *,Func &&,const Extra &...)': expects 3 arguments - 1 provided\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include\\pybind11\\pybind11.h(1620): note: see declaration of 'pybind11::class_<fasttext::Vector>::def'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(186): error C2065: 'ssize_t': undeclared identifier\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(186): error C2065: 'ssize_t': undeclared identifier\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(186): error C2672: 'pybind11::init': no matching overloaded function found\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(183): error C2974: 'pybind11::init': invalid template argument for 'CFunc', type expected\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include\\pybind11\\pybind11.h(1978): note: see declaration of 'pybind11::init'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(183): error C2974: 'pybind11::init': invalid template argument for 'Func', type expected\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include\\pybind11\\pybind11.h(1970): note: see declaration of 'pybind11::init'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(183): error C2974: 'pybind11::init': invalid template argument for 'Args', type expected\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include\\pybind11\\pybind11.h(1958): note: see declaration of 'pybind11::init'\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(186): error C2672: 'pybind11::class_<fasttext::DenseMatrix>::def': no matching overloaded function found\n",
      "      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(183): error C2780: 'pybind11::class_<fasttext::DenseMatrix> &pybind11::class_<fasttext::DenseMatrix>::def(const char *,Func &&,const Extra &...)': expects 3 arguments - 1 provided\n",
      "      C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-build-env-g0jxu7n1\\overlay\\Lib\\site-packages\\pybind11\\include\\pybind11\\pybind11.h(1620): note: see declaration of 'pybind11::class_<fasttext::DenseMatrix>::def'\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.29.30133\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for fasttext\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (fasttext)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting txtai (from txtai[pipeline])\n",
      "  Cloning https://github.com/neuml/txtai to c:\\users\\harrison\\appdata\\local\\temp\\pip-install-z2af2c_a\\txtai_11fd328dc41b46e58efe86f892910239\n",
      "  Resolved https://github.com/neuml/txtai to commit bb851a1dc09f63f1a5b2502f50dd182f223461ce\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting faiss-cpu>=1.7.1.post2 (from txtai->txtai[pipeline])\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting msgpack>=1.0.7 (from txtai->txtai[pipeline])\n",
      "  Downloading msgpack-1.1.0-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting torch>=1.12.1 (from txtai->txtai[pipeline])\n",
      "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers>=4.45.0 (from txtai->txtai[pipeline])\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting huggingface-hub>=0.19.0 (from txtai->txtai[pipeline])\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.18.4 (from txtai->txtai[pipeline])\n",
      "  Downloading numpy-2.2.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pyyaml>=5.3 (from txtai->txtai[pipeline])\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex>=2022.8.17 (from txtai->txtai[pipeline])\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting onnx>=1.11.0 (from txtai->txtai[pipeline])\n",
      "  Downloading onnx-1.17.0-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "Collecting onnxruntime>=1.11.0 (from txtai->txtai[pipeline])\n",
      "  Downloading onnxruntime-1.20.1-cp311-cp311-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting scipy>=1.4.1 (from txtai->txtai[pipeline])\n",
      "  Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting sounddevice>=0.5.0 (from txtai->txtai[pipeline])\n",
      "  Using cached sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting soundfile>=0.10.3.post1 (from txtai->txtai[pipeline])\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl.metadata (14 kB)\n",
      "Collecting ttstokenizer>=1.0.0 (from txtai->txtai[pipeline])\n",
      "  Downloading ttstokenizer-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting webrtcvad-wheels>=2.0.14 (from txtai->txtai[pipeline])\n",
      "  Downloading webrtcvad_wheels-2.0.14-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting beautifulsoup4>=4.9.3 (from txtai->txtai[pipeline])\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting docling>=2.8.2 (from txtai->txtai[pipeline])\n",
      "  Using cached docling-2.10.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting nltk>=3.5 (from txtai->txtai[pipeline])\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pandas>=1.1.0 (from txtai->txtai[pipeline])\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting tika>=1.24 (from txtai->txtai[pipeline])\n",
      "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting imagehash>=4.2.1 (from txtai->txtai[pipeline])\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting pillow>=7.1.2 (from txtai->txtai[pipeline])\n",
      "  Using cached pillow-11.0.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting timm>=0.4.12 (from txtai->txtai[pipeline])\n",
      "  Downloading timm-1.0.12-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting litellm>=1.37.16 (from txtai->txtai[pipeline])\n",
      "  Downloading litellm-1.54.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting llama-cpp-python>=0.2.75 (from txtai->txtai[pipeline])\n",
      "  Downloading llama_cpp_python-0.3.5.tar.gz (64.5 MB)\n",
      "     ---------------------------------------- 0.0/64.5 MB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.7/64.5 MB 100.7 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 38.0/64.5 MB 92.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 52.7/64.5 MB 85.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 64.5/64.5 MB 80.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fasttext>=0.9.2 (from txtai->txtai[pipeline])\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting sentencepiece>=0.1.91 (from txtai->txtai[pipeline])\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting accelerate>=0.26.0 (from txtai->txtai[pipeline])\n",
      "  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes>=0.42.0 (from txtai->txtai[pipeline])\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting onnxmltools>=1.9.1 (from txtai->txtai[pipeline])\n",
      "  Downloading onnxmltools-1.12.0-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting peft>=0.8.1 (from txtai->txtai[pipeline])\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting skl2onnx>=1.9.1 (from txtai->txtai[pipeline])\n",
      "  Downloading skl2onnx-1.17.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harrison\\anaconda3\\envs\\brandon\\lib\\site-packages (from accelerate>=0.26.0->txtai->txtai[pipeline]) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\harrison\\anaconda3\\envs\\brandon\\lib\\site-packages (from accelerate>=0.26.0->txtai->txtai[pipeline]) (6.1.0)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.26.0->txtai->txtai[pipeline])\n",
      "  Using cached safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.9.3->txtai->txtai[pipeline])\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in c:\\users\\harrison\\anaconda3\\envs\\brandon\\lib\\site-packages (from bitsandbytes>=0.42.0->txtai->txtai[pipeline]) (4.12.2)\n",
      "Collecting certifi>=2024.7.4 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting deepsearch-glm<2.0.0,>=1.0.0 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached deepsearch_glm-1.0.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting docling-core<3.0.0,>=2.9.0 (from docling-core[chunking]<3.0.0,>=2.9.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached docling_core-2.9.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting docling-ibm-models<3.0.0,>=2.0.6 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached docling_ibm_models-2.0.7-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting docling-parse<4.0.0,>=3.0.0 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached docling_parse-3.0.0-cp311-cp311-win_amd64.whl.metadata (7.5 kB)\n",
      "Collecting easyocr<2.0,>=1.7 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting lxml<6.0.0,>=4.0.0 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached lxml-5.3.0-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached marko-2.1.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting openpyxl<4.0.0,>=3.1.5 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pydantic<3.0.0,>=2.0.0 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.3.0 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx<2.0.0,>=1.0.2 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting requests<3.0.0,>=2.32.3 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached Rtree-1.3.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting typer<0.13.0,>=0.12.5 (from docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pybind11>=2.2 (from fasttext>=0.9.2->txtai->txtai[pipeline])\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\harrison\\anaconda3\\envs\\brandon\\lib\\site-packages (from fasttext>=0.9.2->txtai->txtai[pipeline]) (75.1.0)\n",
      "Collecting filelock (from huggingface-hub>=0.19.0->txtai->txtai[pipeline])\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.19.0->txtai->txtai[pipeline])\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.19.0->txtai->txtai[pipeline])\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting PyWavelets (from imagehash>=4.2.1->txtai->txtai[pipeline])\n",
      "  Downloading pywavelets-1.8.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting aiohttp (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Downloading aiohttp-3.11.10-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting click (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting httpx<0.28.0,>=0.23.0 (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.2 (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting openai>=1.55.3 (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Downloading openai-1.57.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting python-dotenv>=0.2.0 (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting tiktoken>=0.7.0 (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached tiktoken-0.8.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting tokenizers (from litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python>=0.2.75->txtai->txtai[pipeline])\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting joblib (from nltk>=3.5->txtai->txtai[pipeline])\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.11.0->txtai->txtai[pipeline])\n",
      "  Downloading protobuf-5.29.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.11.0->txtai->txtai[pipeline])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.11.0->txtai->txtai[pipeline])\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.11.0->txtai->txtai[pipeline])\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\harrison\\anaconda3\\envs\\brandon\\lib\\site-packages (from pandas>=1.1.0->txtai->txtai[pipeline]) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.0->txtai->txtai[pipeline])\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.0->txtai->txtai[pipeline])\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scikit-learn>=1.1 (from skl2onnx>=1.9.1->txtai->txtai[pipeline])\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting onnxconverter-common>=1.7.0 (from skl2onnx>=1.9.1->txtai->txtai[pipeline])\n",
      "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.5.0->txtai->txtai[pipeline])\n",
      "  Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting torchvision (from timm>=0.4.12->txtai->txtai[pipeline])\n",
      "  Downloading torchvision-0.20.1-cp311-cp311-win_amd64.whl.metadata (6.2 kB)\n",
      "Collecting networkx (from torch>=1.12.1->txtai->txtai[pipeline])\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting sympy (from onnxruntime>=1.11.0->txtai->txtai[pipeline])\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.11.0->txtai->txtai[pipeline])\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting anyascii>=0.3.1 (from ttstokenizer>=1.0.0->txtai->txtai[pipeline])\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting inflect>=0.3.1 (from ttstokenizer>=1.0.0->txtai->txtai[pipeline])\n",
      "  Downloading inflect-7.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.5.0->txtai->txtai[pipeline])\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting pywin32<308,>=307 (from deepsearch-glm<2.0.0,>=1.0.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached pywin32-307-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.9.0->docling-core[chunking]<3.0.0,>=2.9.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pillow>=7.1.2 (from txtai->txtai[pipeline])\n",
      "  Using cached pillow-10.4.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting tabulate<0.10.0,>=0.9.0 (from docling-core<3.0.0,>=2.9.0->docling-core[chunking]<3.0.0,>=2.9.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.9.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached semchunk-2.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<3.0.0,>=2.0.6->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opencv-python-headless<5.0.0.0,>=4.6.0.66 (from docling-ibm-models<3.0.0,>=2.0.6->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting autoflake<3.0.0,>=2.3.1 (from docling-parse<4.0.0,>=3.0.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached autoflake-2.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting scikit-image (from easyocr<2.0,>=1.7->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Downloading scikit_image-0.24.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting python-bidi (from easyocr<2.0,>=1.7->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached python_bidi-0.6.3-cp311-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting Shapely (from easyocr<2.0,>=1.7->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached shapely-2.0.6-cp311-cp311-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting pyclipper (from easyocr<2.0,>=1.7->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached pyclipper-1.3.0.post6-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting ninja (from easyocr<2.0,>=1.7->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached ninja-1.11.1.2-py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.23.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.23.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<0.28.0,>=0.23.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.23.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting more-itertools>=8.5.0 (from inflect>=0.3.1->ttstokenizer>=1.0.0->txtai->txtai[pipeline])\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting typeguard>=4.0.1 (from inflect>=0.3.1->ttstokenizer>=1.0.0->txtai->txtai[pipeline])\n",
      "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2<4.0.0,>=3.1.2->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached rpds_py-0.22.3-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.11.0->txtai->txtai[pipeline])\n",
      "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.55.3->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.55.3->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Downloading jiter-0.8.2-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting et-xmlfile (from openpyxl<4.0.0,>=3.1.5->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.0.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3.0.0,>=2.0.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached pydantic_core-2.27.1-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\harrison\\anaconda3\\envs\\brandon\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.0->txtai->txtai[pipeline]) (1.17.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.32.3->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.32.3->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1->skl2onnx>=1.9.1->txtai->txtai[pipeline])\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\harrison\\anaconda3\\envs\\brandon\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->txtai->txtai[pipeline]) (0.4.6)\n",
      "Collecting shellingham>=1.3.0 (from typer<0.13.0,>=0.12.5->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<0.13.0,>=0.12.5->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->litellm>=1.37.16->txtai->txtai[pipeline])\n",
      "  Using cached yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.11.0->txtai->txtai[pipeline])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pyflakes>=3.0.0 (from autoflake<3.0.0,>=2.3.1->docling-parse<4.0.0,>=3.0.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.11.0->txtai->txtai[pipeline])\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<0.13.0,>=0.12.5->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\harrison\\anaconda3\\envs\\brandon\\lib\\site-packages (from rich>=10.11.0->typer<0.13.0,>=0.12.5->docling>=2.8.2->txtai->txtai[pipeline]) (2.18.0)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.9.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting imageio>=2.33 (from scikit-image->easyocr<2.0,>=1.7->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached imageio-2.36.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->easyocr<2.0,>=1.7->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached tifffile-2024.9.20-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->easyocr<2.0,>=1.7->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.13.0,>=0.12.5->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting multiprocess>=0.70.15 (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.9.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting dill>=0.3.9 (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.9.0->docling>=2.8.2->txtai->txtai[pipeline])\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading accelerate-1.2.0-py3-none-any.whl (336 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-win_amd64.whl (68.5 MB)\n",
      "   ---------------------------------------- 0.0/68.5 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 20.2/68.5 MB 98.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 28.3/68.5 MB 69.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 34.6/68.5 MB 56.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 40.4/68.5 MB 50.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 45.4/68.5 MB 43.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 53.5/68.5 MB 43.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 61.1/68.5 MB 42.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  68.4/68.5 MB 41.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 68.5/68.5 MB 40.4 MB/s eta 0:00:00\n",
      "Using cached docling-2.10.0-py3-none-any.whl (96 kB)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp311-cp311-win_amd64.whl (13.8 MB)\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 7.6/13.8 MB 36.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.8/13.8 MB 37.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "Downloading litellm-1.54.1-py3-none-any.whl (6.4 MB)\n",
      "   ---------------------------------------- 0.0/6.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.4/6.4 MB 44.2 MB/s eta 0:00:00\n",
      "Downloading msgpack-1.1.0-cp311-cp311-win_amd64.whl (74 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading numpy-2.2.0-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 8.4/12.9 MB 40.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 38.6 MB/s eta 0:00:00\n",
      "Downloading onnx-1.17.0-cp311-cp311-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 10.5/14.5 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 43.5 MB/s eta 0:00:00\n",
      "Downloading onnxmltools-1.12.0-py2.py3-none-any.whl (329 kB)\n",
      "Downloading onnxruntime-1.20.1-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 8.7/11.3 MB 44.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 41.8 MB/s eta 0:00:00\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 991.5/991.5 kB 48.6 MB/s eta 0:00:00\n",
      "Downloading skl2onnx-1.17.0-py2.py3-none-any.whl (298 kB)\n",
      "Using cached sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Downloading timm-1.0.12-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 33.2 MB/s eta 0:00:00\n",
      "Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.9/203.1 MB 21.3 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 8.9/203.1 MB 22.1 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 15.2/203.1 MB 25.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 22.5/203.1 MB 28.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 27.5/203.1 MB 29.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 28.0/203.1 MB 24.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 33.6/203.1 MB 23.7 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 40.1/203.1 MB 24.7 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 47.4/203.1 MB 26.0 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 55.8/203.1 MB 27.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 62.9/203.1 MB 28.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 64.5/203.1 MB 26.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 71.8/203.1 MB 26.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 76.0/203.1 MB 26.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 83.9/203.1 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 92.8/203.1 MB 28.3 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 102.0/203.1 MB 29.3 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 110.4/203.1 MB 30.0 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 117.7/203.1 MB 30.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 126.6/203.1 MB 31.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 136.1/203.1 MB 31.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 141.3/203.1 MB 31.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 151.8/203.1 MB 32.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 161.5/203.1 MB 33.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 170.9/203.1 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 180.1/203.1 MB 34.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.0/203.1 MB 34.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 195.6/203.1 MB 34.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 34.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 34.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 32.4 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 9.4/10.1 MB 45.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 42.0 MB/s eta 0:00:00\n",
      "Downloading ttstokenizer-1.0.0-py3-none-any.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.1/3.1 MB 45.3 MB/s eta 0:00:00\n",
      "Downloading webrtcvad_wheels-2.0.14-cp311-cp311-win_amd64.whl (19 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Using cached deepsearch_glm-1.0.0-cp311-cp311-win_amd64.whl (7.9 MB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached docling_core-2.9.0-py3-none-any.whl (87 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Using cached docling_ibm_models-2.0.7-py3-none-any.whl (65 kB)\n",
      "Using cached docling_parse-3.0.0-cp311-cp311-win_amd64.whl (23.2 MB)\n",
      "Using cached easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Downloading inflect-7.4.0-py3-none-any.whl (34 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached lxml-5.3.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Using cached marko-2.1.2-py3-none-any.whl (42 kB)\n",
      "Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
      "Downloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
      "Downloading openai-1.57.2-py3-none-any.whl (389 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Using cached pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Using cached pydantic_core-2.27.1-cp311-none-win_amd64.whl (2.0 MB)\n",
      "Using cached pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Using cached pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached Rtree-1.3.0-py3-none-win_amd64.whl (377 kB)\n",
      "Using cached safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Downloading scikit_learn-1.6.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 47.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 46.5 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached tiktoken-0.8.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 34.3 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.20.1-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 41.9 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading aiohttp-3.11.10-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 47.1 MB/s eta 0:00:00\n",
      "Downloading pywavelets-1.8.0-cp311-cp311-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.2/4.2 MB 50.7 MB/s eta 0:00:00\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached autoflake-2.3.1-py3-none-any.whl (32 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading jiter-0.8.2-cp311-cp311-win_amd64.whl (206 kB)\n",
      "Using cached jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Using cached propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached pywin32-307-cp311-cp311-win_amd64.whl (6.5 MB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached rpds_py-0.22.3-cp311-cp311-win_amd64.whl (231 kB)\n",
      "Using cached semchunk-2.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Using cached yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Using cached ninja-1.11.1.2-py3-none-win_amd64.whl (296 kB)\n",
      "Using cached pyclipper-1.3.0.post6-cp311-cp311-win_amd64.whl (110 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached python_bidi-0.6.3-cp311-none-win_amd64.whl (157 kB)\n",
      "Downloading scikit_image-0.24.0-cp311-cp311-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 7.1/12.8 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 38.4 MB/s eta 0:00:00\n",
      "Using cached shapely-2.0.6-cp311-cp311-win_amd64.whl (1.4 MB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached imageio-2.36.1-py3-none-any.whl (315 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
      "Using cached tifffile-2024.9.20-py3-none-any.whl (228 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
      "Using cached mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "Using cached dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Building wheels for collected packages: txtai, fasttext, llama-cpp-python, tika\n",
      "  Building wheel for txtai (pyproject.toml): started\n",
      "  Building wheel for txtai (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for txtai: filename=txtai-8.2.0-py3-none-any.whl size=260343 sha256=5671f1c26133b3e5dc94ea14de830cb9011c07d6be53c919b0c0517d79fb988e\n",
      "  Stored in directory: C:\\Users\\Harrison\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-rl0lv5xf\\wheels\\d1\\3c\\4d\\9e4fd630fffee8b8fdeadaf4ceacf70b36da2b308c56905650\n",
      "  Building wheel for fasttext (pyproject.toml): started\n",
      "  Building wheel for fasttext (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.5-cp311-cp311-win_amd64.whl size=3330717 sha256=6acfa5d48916e6ef6786da9e3a70b53aaea5f59424c74880eab10fba69c3441b\n",
      "  Stored in directory: c:\\users\\harrison\\appdata\\local\\pip\\cache\\wheels\\12\\8a\\8d\\ea0c2c1a2a663d41431988f6e92de7d7f479d4e95a5c49fe85\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32689 sha256=dc0db8a500340eae11465e57b4e55adb9d2486970509044f7a4b07706ae6df82\n",
      "  Stored in directory: c:\\users\\harrison\\appdata\\local\\pip\\cache\\wheels\\27\\ba\\2f\\37420d1191bdae5e855d69b8e913673045bfd395cbd78ad697\n",
      "Successfully built txtai llama-cpp-python tika\n",
      "Failed to build fasttext\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline]\n",
    "\n",
    "# Get test data\n",
    "!wget -N https://github.com/neuml/txtai/releases/download/v6.2.0/tests.tar.gz\n",
    "!tar -xvzf tests.tar.gz\n",
    "\n",
    "# Install NLTK\n",
    "import nltk\n",
    "nltk.download(['punkt', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNPJ95cdTKSS"
   },
   "source": [
    "# Create a Textractor instance\n",
    "\n",
    "The Textractor instance is the main entrypoint for extracting text. This method is backed by Apache Tika, a robust text extraction library written in Java. [Apache Tika](https://tika.apache.org/0.9/formats.html) has support for a large number of file formats: PDF, Word, Excel, HTML and others. The [Python Tika package](https://github.com/chrismattmann/tika-python) automatically installs Tika and starts a local REST API instance used to read extracted data.\n",
    "\n",
    "*Note: This requires Java to be installed locally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nTDwXOUeTH2-"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from txtai.pipeline import Textractor\n",
    "\n",
    "# Create textractor model\n",
    "textractor = Textractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vGR_piwZZO6"
   },
   "source": [
    "# Extract text\n",
    "\n",
    "The example below shows how to extract text from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "-K2YJJzsVtfq",
    "outputId": "7754c508-264a-41fa-9843-83460719820f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Introducing txtai, an AI-powered search engine \\nbuilt on Transformers\\n\\nAdd Natural Language Understanding to any application\\n\\nSearch is the base of many applications. Once data starts to pile up, users want to be able to find it. It’s \\nthe foundation of the internet and an ever-growing challenge that is never solved or done.\\n\\nThe field of Natural Language Processing (NLP) is rapidly evolving with a number of new \\ndevelopments. Large-scale general language models are an exciting new capability allowing us to add \\namazing functionality quickly with limited compute and people. Innovation continues with new models\\nand advancements coming in at what seems a weekly basis.\\n\\nThis article introduces txtai, an AI-powered search engine that enables Natural Language \\nUnderstanding (NLU) based search in any application.\\n\\nIntroducing txtai\\ntxtai builds an AI-powered index over sections of text. txtai supports building text indices to perform \\nsimilarity searches and create extractive question-answering based systems. txtai also has functionality \\nfor zero-shot classification. txtai is open source and available on GitHub.\\n\\ntxtai and/or the concepts behind it has already been used to power the Natural Language Processing \\n(NLP) applications listed below:\\n\\n• paperai — AI-powered literature discovery and review engine for medical/scientific papers\\n• tldrstory — AI-powered understanding of headlines and story text\\n• neuspo — Fact-driven, real-time sports event and news site\\n• codequestion — Ask coding questions directly from the terminal\\n\\nBuild an Embeddings index\\nFor small lists of texts, the method above works. But for larger repositories of documents, it doesn’t \\nmake sense to tokenize and convert all embeddings for each query. txtai supports building pre-\\ncomputed indices which significantly improves performance.\\n\\nBuilding on the previous example, the following example runs an index method to build and store the \\ntext embeddings. In this case, only the query is converted to an embeddings vector each search.\\n\\nhttps://github.com/neuml/codequestion\\nhttps://neuspo.com/\\nhttps://github.com/neuml/tldrstory\\nhttps://github.com/neuml/paperai\\n - Introducing txtai, an AI-powered search engine built on Transformers\\n - Add Natural Language Understanding to any application\\n - Introducing txtai\\n - Build an Embeddings index'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textractor(\"txtai/article.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2jndgE-JyWX"
   },
   "source": [
    "Note that the text from the article was extracted into a single string. Depending on the articles, this may be acceptable. For long articles, often you'll want to split the content into logical sections to build better downstream vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w2bhBCPOUdu"
   },
   "source": [
    "# Extract sentences\n",
    "\n",
    "Sentence extraction uses a model that specializes in sentence detection. This call returns a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKZVK5vuOTqB",
    "outputId": "a31e182e-037b-4e29-c1b0-0f6815e3b2c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introducing txtai, an AI-powered search engine \\nbuilt on Transformers\\n\\nAdd Natural Language Understanding to any application\\n\\nSearch is the base of many applications.',\n",
       " 'Once data starts to pile up, users want to be able to find it.',\n",
       " 'It’s \\nthe foundation of the internet and an ever-growing challenge that is never solved or done.',\n",
       " 'The field of Natural Language Processing (NLP) is rapidly evolving with a number of new \\ndevelopments.',\n",
       " 'Large-scale general language models are an exciting new capability allowing us to add \\namazing functionality quickly with limited compute and people.',\n",
       " 'Innovation continues with new models\\nand advancements coming in at what seems a weekly basis.',\n",
       " 'This article introduces txtai, an AI-powered search engine that enables Natural Language \\nUnderstanding (NLU) based search in any application.',\n",
       " 'Introducing txtai\\ntxtai builds an AI-powered index over sections of text.',\n",
       " 'txtai supports building text indices to perform \\nsimilarity searches and create extractive question-answering based systems.',\n",
       " 'txtai also has functionality \\nfor zero-shot classification.',\n",
       " 'txtai is open source and available on GitHub.',\n",
       " 'txtai and/or the concepts behind it has already been used to power the Natural Language Processing \\n(NLP) applications listed below:\\n\\n• paperai — AI-powered literature discovery and review engine for medical/scientific papers\\n• tldrstory — AI-powered understanding of headlines and story text\\n• neuspo — Fact-driven, real-time sports event and news site\\n• codequestion — Ask coding questions directly from the terminal\\n\\nBuild an Embeddings index\\nFor small lists of texts, the method above works.',\n",
       " 'But for larger repositories of documents, it doesn’t \\nmake sense to tokenize and convert all embeddings for each query.',\n",
       " 'txtai supports building pre-\\ncomputed indices which significantly improves performance.',\n",
       " 'Building on the previous example, the following example runs an index method to build and store the \\ntext embeddings.',\n",
       " 'In this case, only the query is converted to an embeddings vector each search.',\n",
       " 'https://github.com/neuml/codequestion\\nhttps://neuspo.com/\\nhttps://github.com/neuml/tldrstory\\nhttps://github.com/neuml/paperai\\n - Introducing txtai, an AI-powered search engine built on Transformers\\n - Add Natural Language Understanding to any application\\n - Introducing txtai\\n - Build an Embeddings index']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textractor = Textractor(sentences=True)\n",
    "textractor(\"txtai/article.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdVCCc9UOv5S"
   },
   "source": [
    "Now the document is split up at the sentence level. These sentences can be feed to a workflow that adds each sentence to an embeddings index. Depending on the task, this may work well. Alternatively, it may be even better to split at the paragraph level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1H8XYkaSoP4"
   },
   "source": [
    "# Extract paragraphs\n",
    "\n",
    "Paragraph detection looks for consecutive newlines. This call returns a list of paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VUito4ISoAe",
    "outputId": "08079380-a7c8-4886-ecc3-de9f02be4584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing txtai, an AI-powered search engine \n",
      "built on Transformers \n",
      "----\n",
      "Add Natural Language Understanding to any application \n",
      "----\n",
      "Search is the base of many applications. Once data starts to pile up, users want to be able to find it. It’s \n",
      "the foundation of the internet and an ever-growing challenge that is never solved or done. \n",
      "----\n",
      "The field of Natural Language Processing (NLP) is rapidly evolving with a number of new \n",
      "developments. Large-scale general language models are an exciting new capability allowing us to add \n",
      "amazing functionality quickly with limited compute and people. Innovation continues with new models\n",
      "and advancements coming in at what seems a weekly basis. \n",
      "----\n",
      "This article introduces txtai, an AI-powered search engine that enables Natural Language \n",
      "Understanding (NLU) based search in any application. \n",
      "----\n",
      "Introducing txtai\n",
      "txtai builds an AI-powered index over sections of text. txtai supports building text indices to perform \n",
      "similarity searches and create extractive question-answering based systems. txtai also has functionality \n",
      "for zero-shot classification. txtai is open source and available on GitHub. \n",
      "----\n",
      "txtai and/or the concepts behind it has already been used to power the Natural Language Processing \n",
      "(NLP) applications listed below: \n",
      "----\n",
      "• paperai — AI-powered literature discovery and review engine for medical/scientific papers\n",
      "• tldrstory — AI-powered understanding of headlines and story text\n",
      "• neuspo — Fact-driven, real-time sports event and news site\n",
      "• codequestion — Ask coding questions directly from the terminal \n",
      "----\n",
      "Build an Embeddings index\n",
      "For small lists of texts, the method above works. But for larger repositories of documents, it doesn’t \n",
      "make sense to tokenize and convert all embeddings for each query. txtai supports building pre-\n",
      "computed indices which significantly improves performance. \n",
      "----\n",
      "Building on the previous example, the following example runs an index method to build and store the \n",
      "text embeddings. In this case, only the query is converted to an embeddings vector each search. \n",
      "----\n",
      "https://github.com/neuml/codequestion\n",
      "https://neuspo.com/\n",
      "https://github.com/neuml/tldrstory\n",
      "https://github.com/neuml/paperai\n",
      " - Introducing txtai, an AI-powered search engine built on Transformers\n",
      " - Add Natural Language Understanding to any application\n",
      " - Introducing txtai\n",
      " - Build an Embeddings index \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "textractor = Textractor(paragraphs=True)\n",
    "for paragraph in textractor(\"txtai/article.pdf\"):\n",
    "  print(paragraph, \"\\n----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae6dRQ2LvN-w"
   },
   "source": [
    "# Extract sections\n",
    "\n",
    "Section extraction is format dependent. If page breaks are available, each section is a page. Otherwise, this call returns logical sections such by headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQ6ev2UMwqnh",
    "outputId": "3d45491d-3547-4218-d30c-ba0c4d161256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing txtai, an AI-powered search engine \n",
      "built on Transformers\n",
      "\n",
      "Add Natural Language Understanding to any application\n",
      "\n",
      "Search is the base of many applications. Once data starts to pile up, users want to be able to find it. It’s \n",
      "the foundation of the internet and an ever-growing challenge that is never solved or done.\n",
      "\n",
      "The field of Natural Language Processing (NLP) is rapidly evolving with a number of new \n",
      "developments. Large-scale general language models are an exciting new capability allowing us to add \n",
      "amazing functionality quickly with limited compute and people. Innovation continues with new models\n",
      "and advancements coming in at what seems a weekly basis.\n",
      "\n",
      "This article introduces txtai, an AI-powered search engine that enables Natural Language \n",
      "Understanding (NLU) based search in any application.\n",
      "\n",
      "Introducing txtai\n",
      "txtai builds an AI-powered index over sections of text. txtai supports building text indices to perform \n",
      "similarity searches and create extractive question-answering based systems. txtai also has functionality \n",
      "for zero-shot classification. txtai is open source and available on GitHub.\n",
      "\n",
      "txtai and/or the concepts behind it has already been used to power the Natural Language Processing \n",
      "(NLP) applications listed below:\n",
      "\n",
      "• paperai — AI-powered literature discovery and review engine for medical/scientific papers\n",
      "• tldrstory — AI-powered understanding of headlines and story text\n",
      "• neuspo — Fact-driven, real-time sports event and news site\n",
      "• codequestion — Ask coding questions directly from the terminal\n",
      "\n",
      "Build an Embeddings index\n",
      "For small lists of texts, the method above works. But for larger repositories of documents, it doesn’t \n",
      "make sense to tokenize and convert all embeddings for each query. txtai supports building pre-\n",
      "computed indices which significantly improves performance.\n",
      "\n",
      "Building on the previous example, the following example runs an index method to build and store the \n",
      "text embeddings. In this case, only the query is converted to an embeddings vector each search.\n",
      "\n",
      "https://github.com/neuml/codequestion\n",
      "https://neuspo.com/\n",
      "https://github.com/neuml/tldrstory\n",
      "https://github.com/neuml/paperai\n",
      "[PAGE BREAK]\n",
      "- Introducing txtai, an AI-powered search engine built on Transformers\n",
      " - Add Natural Language Understanding to any application\n",
      " - Introducing txtai\n",
      " - Build an Embeddings index\n"
     ]
    }
   ],
   "source": [
    "textractor = Textractor(sections=True)\n",
    "print(\"\\n[PAGE BREAK]\\n\".join(section for section in textractor(\"txtai/article.pdf\")))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "brandon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
